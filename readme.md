# ğŸš€ Swiggy Delivery Time Prediction

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.10+-blue.svg" alt="Python">
  <img src="https://img.shields.io/badge/scikit--learn-1.3+-orange.svg" alt="sklearn">
  <img src="https://img.shields.io/badge/MLflow-2.8+-purple.svg" alt="MLflow">
  <img src="https://img.shields.io/badge/Pipeline-Architecture-green.svg" alt="Pipeline">
</p>

> **Production-grade ML pipeline** for predicting food delivery times using **method chaining**, **type-hinted functional programming**, and **experiment tracking**. Built with modern MLOps practices.

---

## âœ¨ What Makes This Different

| Feature | Implementation | Impact |
|---------|---------------|--------|
| **ğŸ”§ Method Chaining** | `df.pipe(clean).pipe(feature_engineer).pipe(model)` | Readable, testable, no intermediate variables |
| **ğŸ“ Type Hints** | `def clean_data(data: pd.DataFrame) -> pd.DataFrame` | Self-documenting, IDE-friendly, fewer bugs |
| **âš¡ Functional Pipelines** | Pure functions, no side effects | Reproducible, unit-testable components |
| **ğŸ¯ Missing Value Intelligence** | `MissingIndicator` + KNN imputation | Captures missingness patterns as features |
| **ğŸ“Š MLflow Integration** | Experiment tracking & model registry | Production MLOps ready |

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Raw Data      â”‚â”€â”€â”€â”€â–¶â”‚  Data Cleaning   â”‚â”€â”€â”€â”€â–¶â”‚  Feature Eng  â”‚
â”‚  (45,593 rows)  â”‚     â”‚  (Method Chain)  â”‚     â”‚  (Haversine +  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚   Time Features)â”‚
                                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                          â”‚
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Preprocessing   â”‚
                    â”‚  Pipeline        â”‚
                    â”‚                  â”‚
                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                    â”‚  â”‚ Simple     â”‚  â”‚  â—„â”€â”€ Mode/Missing imputation
                    â”‚  â”‚ Imputer    â”‚  â”‚
                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                    â”‚        â”‚          â”‚
                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                    â”‚  â”‚ Column     â”‚  â”‚  â—„â”€â”€ OneHot + Ordinal encoding
                    â”‚  â”‚ Transformerâ”‚  â”‚
                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                    â”‚        â”‚          â”‚
                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                    â”‚  â”‚ KNN        â”‚  â”‚  â—„â”€â”€ Distance-based imputation
                    â”‚  â”‚ Imputer    â”‚  â”‚
                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Model Training  â”‚
                    â”‚  (RF/XGB/Light)  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§¹ Data Cleaning Pipeline (Method Chaining)

```python
# No messy intermediate variables. Pure flow.
cleaned_df = (
    df
    .pipe(drop_anomalies)           # Remove 38 minors + 53 six-star ratings
    .pipe(handle_hidden_nans)        # "NaN " â†’ np.nan (8,515 values!)
    .pipe(clean_coordinates)        # Absolute values + threshold filtering
    .pipe(extract_datetime_features) # order_date â†’ day/month/weekend/time_of_day
    .pipe(calculate_haversine)       # Restaurant â†” Delivery distance
    .pipe(rename_columns)           # snake_case, descriptive names
)
```

**Key Insight:** Discovered `"NaN "` (with trailing space) as hidden missing values â€” a classic real-world data quality issue.

---

## ğŸ”¬ Exploratory Data Analysis

### Statistical Rigor
- **Chi-squared tests** for categorical associations (`festival` â†” `traffic`: p < 0.001)
- **ANOVA** for numerical-categorical relationships
- **Jarque-Bera** normality testing on target variable

### Feature Engineering
| Feature | Method | Business Logic |
|---------|--------|----------------|
| `distance` | Haversine formula | Actual delivery distance in km |
| `pickup_time_minutes` | `order_picked - order_time` | Restaurant preparation time |
| `distance_type` | `pd.cut()` bins | Ordinal: short â†’ very_long |
| `is_weekend` | `dt.day_name().isin([Sat,Sun])` | Weekend demand patterns |
| `order_time_of_day` | Custom `np.select()` | Morning/Afternoon/Evening/Night |

---

## âš™ï¸ Preprocessing Pipeline

```python
processing_pipeline = Pipeline([
    ("simple_imputer", ColumnTransformer([
        ("mode_imputer", SimpleImputer(strategy="most_frequent", add_indicator=True), 
         ['multiple_deliveries', 'festival', 'city_type']),
        ("missing_imputer", SimpleImputer(strategy="constant", fill_value="missing", add_indicator=True),
         ['weather', 'type_of_order', 'type_of_vehicle', 'is_weekend', 'order_time_of_day'])
    ], remainder="passthrough")),
    
    ("preprocess", ColumnTransformer([
        ("scale", MinMaxScaler(), num_cols),
        ("nominal_encode", OneHotEncoder(drop="first", sparse_output=False), nominal_cat_cols),
        ("ordinal_encode", OrdinalEncoder(categories=[traffic_order, distance_type_order],
                                          encoded_missing_value=-999), ordinal_cat_cols)
    ], remainder="passthrough")),
    
    ("knn_imputer", KNNImputer(n_neighbors=5))  # Final polish on remaining NaNs
])
```

**Innovation:** `add_indicator=True` captures *which* values were missing â€” often predictive!

---

## ğŸ“ˆ Model Performance

| Model | Train MAE | Test MAE | RÂ² (Test) | CV RÂ² (5-fold) |
|-------|-----------|----------|-----------|----------------|
| Linear Regression | 4.83 min | 4.86 min | 0.58 | - |
| **Random Forest** | **1.22 min** | **3.29 min** | **0.80** | **0.784 Â± 0.003** |

> **Target Transformation:** Yeo-Johnson PowerTransformer on `time_taken` for normality.

---

## ğŸ§ª Experiment Tracking (MLflow)

```python
with mlflow.start_run(run_name="Missing Indicator + KNN"):
    mlflow.log_param("experiment_type", "Advanced Imputation")
    mlflow.log_params(model.get_params())
    mlflow.log_metric("test_mae", 3.29)
    mlflow.log_metric("cv_r2", 0.784)
    # Full reproducibility: params, metrics, artifacts, model version
```

---

## ğŸš€ Quick Start

```bash
# Clone & setup
git clone https://github.com/yourusername/swiggy-delivery-prediction.git
cd swiggy-delivery-prediction
pip install -r requirements.txt

# Start MLflow tracking server
mlflow ui --port 5000

# Run pipeline
python src/train.py --model random_forest --track-experiments
```

---

## ğŸ“ Project Structure

```
swiggy-delivery-prediction/
â”œâ”€â”€ ğŸ“‚ data/
â”‚   â”œâ”€â”€ raw/swiggy.csv                    # Original 45K records
â”‚   â””â”€â”€ processed/swiggy_cleaned.csv      # Post-method-chain
â”œâ”€â”€ ğŸ“‚ src/
â”‚   â”œâ”€â”€ data_clean_utils.py               # ğŸ”§ Method chaining core
â”‚   â”œâ”€â”€ features.py                       # Haversine + time features
â”‚   â”œâ”€â”€ pipeline.py                       # sklearn Pipeline definitions
â”‚   â””â”€â”€ train.py                          # Entry point with MLflow
â”œâ”€â”€ ğŸ“‚ notebooks/
â”‚   â”œâ”€â”€ 01_data_cleaning.ipynb            # EDA + anomaly detection
â”‚   â”œâ”€â”€ 02_feature_engineering.ipynb      # Method chaining demo
â”‚   â””â”€â”€ 03_model_training.ipynb           # Pipeline + tuning
â”œâ”€â”€ ğŸ“‚ tests/
â”‚   â””â”€â”€ test_pipelines.py                 # Unit tests for pure functions
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---

## ğŸ“ Key Learnings

| Challenge | Solution | Takeaway |
|-----------|----------|----------|
| Hidden string NaNs | Regex detection + `replace("NaN ", np.nan)` | Always inspect `df.sample(50)` |
| 17% missing data | Missing indicators + KNN imputation | Missingness is information |
| 4,071 invalid coordinates | Absolute values + threshold to NaN | Domain knowledge > statistics |
| Target bimodality | Yeo-Johnson transformation | Check distributions before modeling |

---

## ğŸ”® Next Steps

- [ ] **Hyperparameter tuning:** `Optuna` for RF/XGB/LightGBM
- [ ] **Feature selection:** `SelectKBest` + `RFE` on 50+ features
- [ ] **Model stacking:** Ensemble of tree-based models
- [ ] **SHAP interpretability:** Explain delivery time drivers
- [ ] **API deployment:** FastAPI + Docker containerization

---

## ğŸ› ï¸ Tech Stack

<p align="left">
  <img src="https://img.shields.io/badge/pandas-150458?style=for-the-badge&logo=pandas&logoColor=white" />
  <img src="https://img.shields.io/badge/scikit--learn-F7931E?style=for-the-badge&logo=scikit-learn&logoColor=white" />
  <img src="https://img.shields.io/badge/NumPy-013243?style=for-the-badge&logo=numpy&logoColor=white" />
  <img src="https://img.shields.io/badge/MLflow-0194E2?style=for-the-badge&logo=mlflow&logoColor=white" />
  <img src="https://img.shields.io/badge/seaborn-3793EF?style=for-the-badge&logoColor=white" />
</p>

---

<p align="center">
  <b>Built with method chaining, type safety, and MLOps best practices.</b><br>
  <i>â­ Star if you find the pipeline architecture useful!</i>
</p>
```